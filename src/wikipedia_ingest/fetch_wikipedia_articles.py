# file: src/wikipedia_ingest/fetch_wikipedia_articles.py
import os
import sys
import logging
from pathlib import Path
from typing import Any, Dict, List
from urllib.parse import quote

import requests
from supabase import create_client, Client
from dotenv import load_dotenv  # ðŸ‘ˆ Ð”ÐžÐ‘ÐÐ’Ð˜Ð›Ð˜ Ð­Ð¢Ðž

# ==========
# Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
# ==========

log = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ÐŸÐ¾Ð´Ñ‚ÑÐ³Ð¸Ð²Ð°ÐµÐ¼ .env (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾ Ð¸ Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€Ðµ, ÐºÐ°Ðº Ð² telegram_ingest)
load_dotenv()
# ==========
# ÐŸÑƒÑ‚Ð¸ Ðº Ð¾Ð±Ñ‰ÐµÐ¼Ñƒ ÐºÐ¾Ð´Ñƒ (ÐºÐ°Ðº Ð² telegram_ingest)
# ==========

CURRENT_DIR = Path(__file__).resolve()
SRC_DIR = CURRENT_DIR.parents[1]  # .../src
if str(SRC_DIR) not in sys.path:
    sys.path.append(str(SRC_DIR))

from webapp_backend.openai_client import normalize_telegram_post  # ðŸ‘ˆ Ð’ÐÐ–ÐÐÐ¯ Ð¡Ð¢Ð ÐžÐšÐ
from webapp_backend.cards_service import _insert_cards_into_db    # ðŸ‘ˆ Ð¸ ÑÑ‚Ð° Ñ‚Ð¾Ð¶Ðµ
# ==========
# Supabase
# ==========

SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_KEY = os.environ["SUPABASE_KEY"]

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ==========
# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Wikipedia
# ==========

# Ð–Ñ‘ÑÑ‚ÐºÐ¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð¿Ð¾ Ð½Ð°ÑˆÐ¸Ð¼ Ñ‚ÐµÐ¼Ð°Ð¼.
# ÐŸÐ¾Ñ‚Ð¾Ð¼ Ð¼Ð¾Ð¶Ð½Ð¾ Ð±ÑƒÐ´ÐµÑ‚ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð° Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð±Ð¾Ð»ÐµÐµ ÑƒÐ¼Ð½Ð¾Ðµ.
WIKIPEDIA_ARTICLES: Dict[str, List[Dict[str, Any]]] = {
    "en": [
        {"title": "Artificial intelligence", "tags": ["tech", "world_news"]},
        {"title": "Startup company", "tags": ["business", "careers"]},
        {
            "title": "Higher education in the United Kingdom",
            "tags": ["uk_students", "education"],
        },
        {"title": "Streaming media", "tags": ["entertainment", "movies"]},
        {"title": "Climate change", "tags": ["world_news", "society"]},
    ],
    "ru": [
        {"title": "Ð˜ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚", "tags": ["tech", "world_news", "russia"]},
        {"title": "Ð¡Ñ‚Ð°Ñ€Ñ‚Ð°Ð¿", "tags": ["business", "careers"]},
        {
            "title": "Ð’Ñ‹ÑÑˆÐµÐµ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² Ð’ÐµÐ»Ð¸ÐºÐ¾Ð±Ñ€Ð¸Ñ‚Ð°Ð½Ð¸Ð¸",
            "tags": ["uk_students", "education"],
        },
        {"title": "Ð¡Ñ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³", "tags": ["entertainment", "movies"]},
        {"title": "Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ ÐºÐ»Ð¸Ð¼Ð°Ñ‚Ð°", "tags": ["world_news", "society"]},
    ],
}

# Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ Ð±Ñ€Ð°Ñ‚ÑŒ Ð½Ð° Ð¾Ð´Ð¸Ð½ ÑÐ·Ñ‹Ðº Ð·Ð° Ð·Ð°Ð¿ÑƒÑÐº
WIKIPEDIA_MAX_PER_LANG = int(os.getenv("WIKIPEDIA_MAX_PER_LANG", "5"))

USER_AGENT = os.getenv(
    "WIKIPEDIA_USER_AGENT",
    "EYYE-NewsBot/0.1 (https://example.com; contact@example.com)",
)


def _build_wikipedia_url(lang: str, title: str) -> str:
    encoded = quote(title.replace(" ", "_"))
    return f"https://{lang}.wikipedia.org/wiki/{encoded}"


def _fetch_page_summary(lang: str, title: str) -> Dict[str, Any] | None:
    """
    Ð‘ÐµÑ€Ñ‘Ð¼ summary ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¸Ð· REST API Wikipedia.
    """
    encoded = quote(title.replace(" ", "_"))
    url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{encoded}"

    try:
        resp = requests.get(
            url,
            headers={"User-Agent": USER_AGENT},
            timeout=10,
        )
    except Exception:
        log.exception("Failed to call Wikipedia API for %s (%s)", title, lang)
        return None

    if resp.status_code != 200:
        log.warning(
            "Wikipedia API returned %s for %s (%s)", resp.status_code, title, lang
        )
        return None

    data = resp.json()
    extract = (data.get("extract") or "").strip()
    if not extract:
        return None

    page_title = (data.get("title") or title).strip()
    content_urls = data.get("content_urls") or {}
    desktop = content_urls.get("desktop") or {}
    page_url = desktop.get("page") or _build_wikipedia_url(lang, page_title)

    return {
        "title": page_title,
        "extract": extract,
        "url": page_url,
        "lang": lang,
    }


def _merge_tags(base_tags: List[str], model_tags: Any) -> List[str]:
    """
    ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ñ‚ÐµÐ³Ð¸ Ð¸Ð· Ð½Ð°ÑˆÐµÐ¹ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¸Ð· Ð¼Ð¾Ð´ÐµÐ»Ð¸ OpenAI.
    """
    result: List[str] = []

    def _add_many(items: Any):
        nonlocal result
        if not items:
            return
        if isinstance(items, str):
            items = [items]
        if not isinstance(items, list):
            return
        for t in items:
            if not isinstance(t, str):
                continue
            v = t.strip().lower()
            if not v:
                continue
            result.append(v)

    _add_many(base_tags)
    _add_many(model_tags)

    seen = set()
    deduped: List[str] = []
    for t in result:
        if t not in seen:
            seen.add(t)
            deduped.append(t)
    return deduped


def _card_exists(url: str) -> bool:
    """
    ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ ÑƒÐ¶Ðµ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ° Ñ Ñ‚Ð°ÐºÐ¸Ð¼ source_ref (URL).
    """
    resp = (
        supabase.table("cards")
        .select("id")
        .eq("source_type", "wikipedia")
        .eq("source_ref", url)
        .limit(1)
        .execute()
    )
    data = resp.data or []
    return len(data) > 0


def fetch_wikipedia_articles() -> None:
    """
    ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½: Ð·Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑŒÐ¸ Ð¸Ð· Wikipedia Ð¸ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð² cards.
    """
    all_cards: List[Dict[str, Any]] = []

    for lang, articles in WIKIPEDIA_ARTICLES.items():
        log.info("Processing Wikipedia articles for lang=%s", lang)

        for article in articles[:WIKIPEDIA_MAX_PER_LANG]:
            title = article["title"]
            base_tags = article.get("tags") or []

            summary = _fetch_page_summary(lang, title)
            if not summary:
                log.warning("No summary for %s (%s), skipping", title, lang)
                continue

            url = summary["url"]
            if _card_exists(url):
                log.info("Card for %s already exists, skipping", url)
                continue

            raw_text = summary["extract"]

            try:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ñ‚ Ð¶Ðµ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€, Ñ‡Ñ‚Ð¾ Ð¸ Ð´Ð»Ñ Telegram â€” Ð¾Ð½ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ñ‚ÐµÐºÑÑ‚.
                normalized = normalize_telegram_post(
                    raw_text=raw_text,
                    channel_title=f"Wikipedia ({lang})",
                    language=lang,
                )
            except Exception:
                log.exception(
                    "Failed to normalize Wikipedia article %s (%s)", title, lang
                )
                continue

            tags = _merge_tags(base_tags, normalized.get("tags"))
            source_name = (
                (normalized.get("source_name") or "").strip()
                or f"Wikipedia {lang.upper()}"
            )
            language = (normalized.get("language") or "").strip() or lang

            card = {
                "title": (normalized.get("title") or summary["title"]).strip(),
                "body": (normalized.get("body") or raw_text).strip(),
                "tags": tags,
                "importance_score": float(normalized.get("importance_score", 0.4)),
                "language": language,
                "source_name": source_name,
                "source_ref": url,
            }

            log.info(
                "Prepared Wikipedia card: title=%r, source_name=%r, tags=%r",
                card["title"],
                source_name,
                tags,
            )
            all_cards.append(card)

    if not all_cards:
        log.info("No Wikipedia cards prepared on this run")
        return

    # Ð’ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ð°Ñ‡ÐºÐ¾Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ñ‰Ð¸Ð¹ helper (ÐºÐ°Ðº Ð² Telegram ingest)
    inserted = _insert_cards_into_db(
        supabase,
        all_cards,
        language=None,  # ÑÐ·Ñ‹Ðº Ð±ÐµÑ€Ñ‘Ð¼ Ð¸Ð· ÑÐ°Ð¼Ð¸Ñ… ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐµÐº
        source_type="wikipedia",
        fallback_source_name="Wikipedia",
        source_ref=None,
    )
    log.info("Inserted %d Wikipedia cards", len(inserted))


def main() -> None:
    fetch_wikipedia_articles()


if __name__ == "__main__":
    # CLI: PYTHONPATH=src python -m wikipedia_ingest.fetch_wikipedia_articles
    main()
