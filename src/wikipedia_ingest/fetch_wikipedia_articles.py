# file: src/wikipedia_ingest/fetch_wikipedia_articles.py
import os
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests
from dotenv import load_dotenv
from supabase import Client, create_client

# –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ–º .env, —á—Ç–æ–±—ã SUPABASE_*/OPENAI_API_KEY –±—ã–ª–∏ –≤–∏–¥–Ω—ã
load_dotenv()

from webapp_backend.openai_client import normalize_telegram_post

log = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ==========
# Supabase
# ==========

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("SUPABASE_URL or SUPABASE_KEY is not set. Check your .env")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ==========
# –ë–∞–∑–æ–≤—ã–µ —Ç–æ–ø–∏–∫–∏ (–∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ EYYE)
# ==========

ALLOWED_TOPIC_TAGS: List[str] = [
    "world_news",
    "business",
    "finance",
    "tech",
    "science",
    "history",
    "politics",
    "society",
    "entertainment",
    "gaming",
    "sports",
    "lifestyle",
    "education",
]

ALLOWED_TOPIC_TAGS_SET = set(ALLOWED_TOPIC_TAGS)

TAG_SYNONYMS: Dict[str, str] = {
    # —Å–∏–Ω–æ–Ω–∏–º—ã –∏–∑ —Å—Ç–∞—Ä—ã—Ö —Å—Ö–µ–º –∏ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏
    "uk_students": "education",
    "students": "education",
    "student": "education",
    "careers": "education",
    "career": "education",
    "jobs": "education",

    "movies": "entertainment",
    "movie": "entertainment",
    "film": "entertainment",
    "films": "entertainment",
    "tv": "entertainment",
    "television": "entertainment",
    "series": "entertainment",
    "cinema": "entertainment",

    "crypto": "finance",
    "cryptocurrency": "finance",
    "cryptocurrencies": "finance",
    "economy": "finance",
    "markets": "finance",
    "stock_market": "finance",

    "ai": "tech",
    "it": "tech",
    "software": "tech",
    "internet": "tech",

    "war": "world_news",
    "geopolitics": "world_news",
    "russia": "world_news",
    "ukraine": "world_news",
    "usa": "world_news",
    "europe": "world_news",
    "news": "world_news",

    "health": "lifestyle",
    "wellness": "lifestyle",
    "nutrition": "lifestyle",

    "games": "gaming",
    "esports": "gaming",

    "education": "education",
    "university": "education",
    "universities": "education",
    "school": "education",
    "schools": "education",

    "sport": "sports",
    "football": "sports",
    "soccer": "sports",
    "basketball": "sports",
    "tennis": "sports",
}

# ==========
# –ö–æ–Ω—Ñ–∏–≥ Wikipedia / Wikimedia
# ==========

# –Ø–∑—ã–∫–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ .env: WIKIPEDIA_LANGS=en,ru,de
WIKIPEDIA_LANGS_ENV = os.getenv("WIKIPEDIA_LANGS", "en,ru")
WIKIPEDIA_LANGS: List[str] = [
    lang.strip() for lang in WIKIPEDIA_LANGS_ENV.split(",") if lang.strip()
]
if not WIKIPEDIA_LANGS:
    WIKIPEDIA_LANGS = ["en", "ru"]

# Seed-—Å—Ç–∞—Ç—å–∏ –ø–æ–¥ –∫–∞–∂–¥—ã–π –∏–∑ –±–∞–∑–æ–≤—ã—Ö —Ç–æ–ø–∏–∫–æ–≤
WIKIPEDIA_SEED_ARTICLES: Dict[str, List[str]] = {
    "en": [
        "Artificial_intelligence",              # tech / science
        "Startup_company",                      # business
        "Cryptocurrency",                       # finance
        "Video_game",                           # gaming
        "Association_football",                 # sports
        "Lifestyle_(sociology)",                # lifestyle
        "Universities_in_the_United_Kingdom",   # education / world_news
        "Streaming_media",                      # entertainment / tech
        "Climate_change",                       # science / society / world_news
        "World_politics",                       # politics / world_news
        "History_of_Europe",                    # history / world_news
    ],
    "ru": [
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π_–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
        "–°—Ç–∞—Ä—Ç–∞–ø",
        "–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞",
        "–í–∏–¥–µ–æ–∏–≥—Ä–∞",
        "–§—É—Ç–±–æ–ª",
        "–û–±—Ä–∞–∑_–∂–∏–∑–Ω–∏",
        "–°–∏—Å—Ç–µ–º–∞_–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è_–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏–∏",
        "–ü–æ—Ç–æ–∫–æ–≤–æ–µ_–º—É–ª—å—Ç–∏–º–µ–¥–∏–∞",
        "–ò–∑–º–µ–Ω–µ–Ω–∏–µ_–∫–ª–∏–º–∞—Ç–∞",
        "–ú–∏—Ä–æ–≤–∞—è_–ø–æ–ª–∏—Ç–∏–∫–∞",
        "–ò—Å—Ç–æ—Ä–∏—è_–ï–≤—Ä–æ–ø—ã",
    ],
}

# Fallback-—Ç–µ–≥–∏ –¥–ª—è seed-—Å—Ç–∞—Ç–µ–π ‚Äî —Ç–æ–ª—å–∫–æ –∏–∑ ALLOWED_TOPIC_TAGS
SEED_TITLE_TAGS: Dict[tuple, List[str]] = {
    ("en", "Artificial_intelligence"): ["tech", "science"],
    ("en", "Startup_company"): ["business"],
    ("en", "Cryptocurrency"): ["finance", "tech"],
    ("en", "Video_game"): ["gaming", "entertainment"],
    ("en", "Association_football"): ["sports"],
    ("en", "Lifestyle_(sociology)"): ["lifestyle", "society"],
    ("en", "Universities_in_the_United_Kingdom"): ["education", "world_news"],
    ("en", "Streaming_media"): ["entertainment", "tech"],
    ("en", "Climate_change"): ["science", "world_news", "society"],
    ("en", "World_politics"): ["politics", "world_news"],
    ("en", "History_of_Europe"): ["history", "world_news"],

    ("ru", "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π_–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"): ["tech", "science"],
    ("ru", "–°—Ç–∞—Ä—Ç–∞–ø"): ["business"],
    ("ru", "–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞"): ["finance", "tech"],
    ("ru", "–í–∏–¥–µ–æ–∏–≥—Ä–∞"): ["gaming", "entertainment"],
    ("ru", "–§—É—Ç–±–æ–ª"): ["sports"],
    ("ru", "–û–±—Ä–∞–∑_–∂–∏–∑–Ω–∏"): ["lifestyle", "society"],
    ("ru", "–°–∏—Å—Ç–µ–º–∞_–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è_–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏–∏"): ["education", "world_news"],
    ("ru", "–ü–æ—Ç–æ–∫–æ–≤–æ–µ_–º—É–ª—å—Ç–∏–º–µ–¥–∏–∞"): ["entertainment", "tech"],
    ("ru", "–ò–∑–º–µ–Ω–µ–Ω–∏–µ_–∫–ª–∏–º–∞—Ç–∞"): ["science", "world_news", "society"],
    ("ru", "–ú–∏—Ä–æ–≤–∞—è_–ø–æ–ª–∏—Ç–∏–∫–∞"): ["politics", "world_news"],
    ("ru", "–ò—Å—Ç–æ—Ä–∏—è_–ï–≤—Ä–æ–ø—ã"): ["history", "world_news"],
}

# –ü—Ä–æ–µ–∫—Ç—ã –¥–ª—è Wikimedia API
WIKIMEDIA_PROJECTS: Dict[str, str] = {
    "en": "en.wikipedia.org",
    "ru": "ru.wikipedia.org",
}

# –û–±—ä—ë–º –∏ —Ä–µ–∂–∏–º—ã –∑–∞–±–æ—Ä–∞

# –°–∫–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö trending-—Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ —è–∑—ã–∫ –º—ã –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º
# (–ø–æ—Å–ª–µ —Å–∫–ª–µ–π–∫–∏ —Ç–æ–ø–æ–≤ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π)
WIKIPEDIA_TRENDING_TITLES_PER_LANG = int(
    os.getenv("WIKIPEDIA_TRENDING_TITLES_PER_LANG", "600")
)

# –ó–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –±–µ—Ä—ë–º trending (–∞–≥—Ä–µ–≥–∞—Ü–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤)
WIKIPEDIA_TRENDING_DAYS = int(
    os.getenv("WIKIPEDIA_TRENDING_DAYS", "7")
)

# –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:
# - "bulk": —Ä–∞–∑–æ–≤–æ –¥–æ–±–∏—Ä–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ wiki-–∫–∞—Ä—Ç –¥–æ WIKIPEDIA_BULK_TARGET_TOTAL
# - "daily": –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º, –¥–æ 50 –∫–∞—Ä—Ç –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–æ–ø–∏–∫ –∑–∞ –∑–∞–ø—É—Å–∫
WIKIPEDIA_INGEST_MODE = os.getenv("WIKIPEDIA_INGEST_MODE", "daily").lower()

# –¶–µ–ª—å –¥–ª—è bulk-–∑–∞–ø—É—Å–∫–∞ (—Å–∫–æ–ª—å–∫–æ wiki-–∫–∞—Ä—Ç–æ—á–µ–∫ –≤ —Å—É–º–º–µ —Ö–æ—Ç–∏–º –∏–º–µ—Ç—å)
WIKIPEDIA_BULK_TARGET_TOTAL = int(
    os.getenv("WIKIPEDIA_BULK_TARGET_TOTAL", "1000")
)

# –õ–∏–º–∏—Ç –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ –æ–¥–∏–Ω —Ç–æ–ø–∏–∫ –∑–∞ –æ–¥–∏–Ω daily-–∑–∞–ø—É—Å–∫
WIKIPEDIA_PER_TOPIC_DAILY_LIMIT = int(
    os.getenv("WIKIPEDIA_PER_TOPIC_DAILY_LIMIT", "50")
)

# –û–±—â–∏–π –ª–∏–º–∏—Ç –∫–∞—Ä—Ç–æ—á–µ–∫ –∑–∞ –æ–¥–∏–Ω daily-–∑–∞–ø—É—Å–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 13 * 50 = 650)
WIKIPEDIA_MAX_CARDS_PER_RUN = int(
    os.getenv(
        "WIKIPEDIA_MAX_CARDS_PER_RUN",
        str(len(ALLOWED_TOPIC_TAGS) * WIKIPEDIA_PER_TOPIC_DAILY_LIMIT),
    )
)

WIKIMEDIA_USER_AGENT = os.getenv(
    "WIKIMEDIA_USER_AGENT",
    "EYYE-MVP/0.1 (https://github.com/artemarnautov/eyye-tg-bot; contact: dev@eyye.local)",
)

WIKIMEDIA_TOP_URL_TEMPLATE = (
    "https://wikimedia.org/api/rest_v1/metrics/pageviews/top/"
    "{project}/all-access/{year}/{month}/{day}"
)

WIKIPEDIA_API_URL_TEMPLATE = "https://{lang}.wikipedia.org/w/api.php"

# –°–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤—Å—Ç–∞–≤–ª—è–µ–º –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
SUPABASE_INSERT_BATCH_SIZE = int(os.getenv("WIKIPEDIA_INSERT_BATCH_SIZE", "50"))

# ==========
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==========


def _normalize_tags(raw_tags: List[Any]) -> List[str]:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–≥–∏ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–º—É —Å–ø–∏—Å–∫—É ALLOWED_TOPIC_TAGS.
    - –º–∞–ø–∏–º —á–µ—Ä–µ–∑ TAG_SYNONYMS,
    - –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏,
    - –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 1+ —Ç–µ–≥ (fallback: world_news).
    """
    result: List[str] = []

    for t in raw_tags or []:
        key = str(t or "").strip().lower()
        if not key:
            continue

        tag_id: Optional[str] = None

        if key in ALLOWED_TOPIC_TAGS_SET:
            tag_id = key
        elif key in TAG_SYNONYMS:
            tag_id = TAG_SYNONYMS[key]
        else:
            # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞–º
            if "crypto" in key or "–±–∏—Ç–∫–æ–∏–Ω" in key or "–∫—Ä–∏–ø—Ç–æ" in key:
                tag_id = "finance"
            elif "blockchain" in key:
                tag_id = "finance"
            elif "ai" in key or "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in key:
                tag_id = "tech"
            elif "game" in key or "–∏–≥—Ä" in key:
                tag_id = "gaming"
            elif "sport" in key or "—Å–ø–æ—Ä—Ç" in key or "league" in key:
                tag_id = "sports"
            elif "university" in key or "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç" in key or "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ" in key:
                tag_id = "education"
            elif "history" in key or "–∏—Å—Ç–æ—Ä–∏—è" in key:
                tag_id = "history"
            elif "climate" in key or "–∫–ª–∏–º–∞—Ç" in key:
                tag_id = "science"
            elif "politic" in key or "–ø–æ–ª–∏—Ç–∏–∫" in key:
                tag_id = "politics"
            elif "film" in key or "movie" in key or "cinema" in key or "—Å–µ—Ä–∏–∞–ª" in key:
                tag_id = "entertainment"

        if tag_id and tag_id in ALLOWED_TOPIC_TAGS_SET and tag_id not in result:
            result.append(tag_id)

    if not result:
        # –µ—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å ‚Äì —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –º–∏—Ä/–Ω–æ–≤–æ—Å—Ç–∏
        result = ["world_news"]

    return result


def _card_exists(source_ref: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∫–∞—Ä—Ç–æ—á–∫–∞ —Å —Ç–∞–∫–∏–º source_type/source_ref.
    """
    resp = (
        supabase.table("cards")
        .select("id")
        .eq("source_type", "wikipedia")
        .eq("source_ref", source_ref)
        .limit(1)
        .execute()
    )
    data = getattr(resp, "data", None)
    if data is None:
        data = getattr(resp, "model", None)
    data = data or []
    return len(data) > 0


def _count_existing_wikipedia_cards() -> int:
    """
    –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ wiki-–∫–∞—Ä—Ç–æ—á–µ–∫ —É–∂–µ –µ—Å—Ç—å –≤ –ë–î.
    """
    try:
        resp = (
            supabase.table("cards")
            .select("id", count="exact")
            .eq("source_type", "wikipedia")
            .execute()
        )
    except Exception as e:
        log.warning("Failed to count existing wikipedia cards: %s", e)
        return 0

    cnt = getattr(resp, "count", None)
    if isinstance(cnt, int):
        return cnt

    data = getattr(resp, "data", None)
    if data is None:
        data = getattr(resp, "model", None)
    data = data or []
    return len(data)


def _fetch_trending_for_lang(lang: str) -> List[Dict[str, Any]]:
    """
    –ë–µ—Ä—ë–º —Ç–æ–ø–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–Ω–µ–π –∏–∑ Wikimedia Pageviews API.
    –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –ø–æ title, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—É–º–º–µ views.

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ dict:
    {
      "title": str,
      "views": int,
      "rank": int,
      "is_seed": bool (False)
    }
    """
    project = WIKIMEDIA_PROJECTS.get(lang)
    if not project:
        log.warning("No Wikimedia project configured for lang=%s", lang)
        return []

    aggregated: Dict[str, Dict[str, Any]] = {}
    today = datetime.utcnow()

    for offset in range(1, WIKIPEDIA_TRENDING_DAYS + 1):
        day = today - timedelta(days=offset)
        url = WIKIMEDIA_TOP_URL_TEMPLATE.format(
            project=project,
            year=day.year,
            month=f"{day.month:02d}",
            day=f"{day.day:02d}",
        )

        headers = {
            "User-Agent": WIKIMEDIA_USER_AGENT,
            "accept": "application/json",
        }

        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            log.warning(
                "Failed to fetch trending articles for %s (offset=%d): %s",
                lang,
                offset,
                e,
            )
            continue

        data = resp.json() or {}
        items = data.get("items") or []
        if not items:
            continue

        first_item = items[0] or {}
        articles = first_item.get("articles") or []

        for art in articles:
            title = art.get("article")
            if not isinstance(title, str):
                continue

            # —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –±–µ—Ä—ë–º
            if title.startswith("Special:") or title.startswith("Main_Page"):
                continue

            views = int(art.get("views") or 0)
            rank = int(art.get("rank") or 9999)

            rec = aggregated.get(title)
            if rec is None:
                aggregated[title] = {
                    "title": title,
                    "views": views,
                    "rank": rank,
                    "is_seed": False,
                }
            else:
                rec["views"] += views
                if rank < rec["rank"]:
                    rec["rank"] = rank

    if not aggregated:
        log.warning("No trending articles aggregated for lang=%s", lang)
        return []

    articles = list(aggregated.values())
    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º (desc), –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ ‚Äî –ø–æ –ª—É—á—à–µ–º—É —Ä–∞–Ω–≥—É (asc)
    articles.sort(key=lambda a: (-a["views"], a["rank"]))

    if len(articles) > WIKIPEDIA_TRENDING_TITLES_PER_LANG:
        articles = articles[:WIKIPEDIA_TRENDING_TITLES_PER_LANG]

    log.info(
        "Aggregated %d trending articles for lang=%s over %d days",
        len(articles),
        lang,
        WIKIPEDIA_TRENDING_DAYS,
    )
    return articles


def _build_articles_for_lang(lang: str) -> List[Dict[str, Any]]:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞:
    - seed-—Å—Ç–∞—Ç—å–∏ (–ø–æ–¥ –≤—Å–µ –±–∞–∑–æ–≤—ã–µ —Ç–æ–ø–∏–∫–∏),
    - trending (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –¥–Ω—è–º).
    –§–æ—Ä–º–∞—Ç —ç–ª–µ–º–µ–Ω—Ç–∞:
    {
      "title": str,
      "views": int,
      "rank": int,
      "is_seed": bool,
    }
    """
    articles: List[Dict[str, Any]] = []

    seed_titles = WIKIPEDIA_SEED_ARTICLES.get(lang, [])
    for idx, title in enumerate(seed_titles):
        articles.append(
            {
                "title": title,
                "views": 0,
                "rank": 1000 + idx,
                "is_seed": True,
            }
        )

    trending = _fetch_trending_for_lang(lang)
    title_set = {a["title"] for a in articles}
    for art in trending:
        if art["title"] in title_set:
            continue
        articles.append(art)
        title_set.add(art["title"])

    return articles


def _fetch_article_extract(lang: str, title: str) -> Optional[str]:
    """
    –¢—è–Ω–µ–º –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ Wikipedia API.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º prop=extracts, plaintext, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ –¥–ª–∏–Ω–µ.
    """
    api_url = WIKIPEDIA_API_URL_TEMPLATE.format(lang=lang)

    headers = {
        "User-Agent": WIKIMEDIA_USER_AGENT,
    }

    params = {
        "action": "query",
        "format": "json",
        "prop": "extracts",
        "explaintext": True,
        "exchars": 2000,  # –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–µ—Ä–≤—ã–µ ~2k —Å–∏–º–≤–æ–ª–æ–≤
        "redirects": 1,
        "titles": title,
    }

    try:
        resp = requests.get(api_url, headers=headers, params=params, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        log.warning(
            "Failed to fetch Wikipedia article '%s' (lang=%s): %s",
            title,
            lang,
            e,
        )
        return None

    data = resp.json() or {}
    pages = data.get("query", {}).get("pages", {})
    if not pages:
        log.warning("No pages in Wikipedia response for title=%r, lang=%s", title, lang)
        return None

    page = next(iter(pages.values()))
    extract = page.get("extract")
    if not extract or not str(extract).strip():
        log.warning("Empty extract for title=%r, lang=%s", title, lang)
        return None

    return str(extract)


def _load_global_topic_demand() -> Dict[str, float]:
    """
    –°–º–æ—Ç—Ä–∏–º —Ç–∞–±–ª–∏—Ü—É user_topic_weights –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤–µ—Å–∞ –ø–æ —Ç–µ–≥–∞–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è [0..1] –ø–æ —Ç–µ–≥–∞–º.
    –≠—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–π "–≤–µ–∫—Ç–æ—Ä –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤" –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–º –º—ã –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ–º wiki-–∏–Ω–≥–µ—Å—Ç.
    """
    try:
        resp = supabase.table("user_topic_weights").select("tag,weight").execute()
    except Exception as e:
        log.warning("Failed to load user_topic_weights for wiki ingest: %s", e)
        return {}

    data = getattr(resp, "data", None)
    if data is None:
        data = getattr(resp, "model", None)
    data = data or []

    demand_raw: Dict[str, float] = {}
    for row in data:
        tag = str(row.get("tag") or "").strip()
        if not tag:
            continue
        try:
            w = float(row.get("weight") or 0.0)
        except Exception:
            w = 0.0
        demand_raw[tag] = demand_raw.get(tag, 0.0) + w

    if not demand_raw:
        return {}

    max_val = max(demand_raw.values())
    if max_val <= 0:
        return {}

    demand_norm = {tag: val / max_val for tag, val in demand_raw.items()}
    log.info("Loaded global topic demand for %d tags", len(demand_norm))
    return demand_norm


def _normalize_to_card(
    lang: str,
    title: str,
    url: str,
    extract: str,
    *,
    popularity_score: float,
    global_topic_demand: Dict[str, float],
) -> Optional[Dict[str, Any]]:
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ–º —Ç–µ–∫—Å—Ç –í–∏–∫–∏ —á–µ—Ä–µ–∑ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π normalize_telegram_post,
    —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å title/body/tags/importance_score.

    –î–æ–ø. –ª–æ–≥–∏–∫–∞:
    - –ø—Ä–∏–≤–æ–¥–∏–º —Ç–µ–≥–∏ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º —Ç–æ–ø–∏–∫–∞–º EYYE;
    - —É—Å–∏–ª–∏–≤–∞–µ–º importance_score –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç:
        * –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø—Ä–æ—Å–∞ –ø–æ —ç—Ç–∏–º —Ç–µ–≥–∞–º (user_topic_weights),
        * –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ wiki-—Å—Ç—Ä–∞–Ω–∏—Ü—ã (pageviews);
    - –≤ meta —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
    """
    normalized = normalize_telegram_post(
        raw_text=extract,
        channel_title=f"Wikipedia ({lang})",
        language=lang,
    )

    raw_tags = normalized.get("tags") or []
    if not isinstance(raw_tags, list):
        raw_tags = [raw_tags]

    tags = _normalize_tags(raw_tags)

    # üîπ fallback-—Ç–µ–≥–∏ –¥–ª—è seed-—Å—Ç—Ä–∞–Ω–∏—Ü, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—à–ª–æ
    if not tags:
        tags = SEED_TITLE_TAGS.get((lang, title), ["world_news"])

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ / —Ç–µ–ª–æ
    norm_title = (normalized.get("title") or "").strip()
    if not norm_title:
        norm_title = title.replace("_", " ")

    norm_body = (normalized.get("body") or "").strip()
    if not norm_body:
        norm_body = extract[:800]

    # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –æ—Ç –º–æ–¥–µ–ª–∏
    try:
        base_importance = float(normalized.get("importance_score", 0.7))
    except Exception:
        base_importance = 0.7

    # –°–ø—Ä–æ—Å –ø–æ —Ç–µ–≥–∞–º: –º–∞–∫—Å–∏–º—É–º –ø–æ –≤—Å–µ–º —Ç–µ–≥–∞–º –∫–∞—Ä—Ç–æ—á–∫–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º –≤–µ–∫—Ç–æ—Ä–µ
    topic_demand_score = 0.0
    for t in tags:
        topic_demand_score = max(topic_demand_score, float(global_topic_demand.get(t, 0.0)))

    # –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å—Ç–∞—Ç—å–∏: —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è [0..1]
    popularity_score = max(0.0, min(float(popularity_score), 1.0))

    # –£—Å–∏–ª–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å:
    # - —Ñ–∞–∫—Ç–æ—Ä –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏: 0.6..1.3
    # - —Ñ–∞–∫—Ç–æ—Ä –ø–æ —Å–ø—Ä–æ—Å—É:      0.7..1.3
    importance = base_importance
    importance *= 0.6 + 0.7 * popularity_score
    importance *= 0.7 + 0.6 * topic_demand_score
    # –ª—ë–≥–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
    if importance < 0.2:
        importance = 0.2
    if importance > 3.0:
        importance = 3.0

    # –ò—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ —Ö–æ—Ç–∏–º –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å–ª–æ–≤–æ "Wikipedia"
    source_name = (normalized.get("source_name") or "").strip()
    if not source_name or "wikipedia" in source_name.lower():
        source_name = "EYYE ‚Ä¢ AI-–ø–æ–¥–±–æ—Ä–∫–∞"

    lang_code = "en" if lang == "en" else "ru"

    meta: Dict[str, Any] = {
        "source_name": source_name,
        "wiki_lang": lang,
        "wiki_title": title,
        "wiki_url": url,
        "wiki_popularity": popularity_score,
        "wiki_topic_demand": topic_demand_score,
    }

    card: Dict[str, Any] = {
        "title": norm_title,
        "body": norm_body,
        "tags": tags,
        "importance_score": importance,
        "language": lang_code,
        "is_active": True,
        "source_type": "wikipedia",
        "source_ref": url,
        "meta": meta,
    }

    log.info(
        "Prepared Wikipedia card: title=%r, tags=%r, importance=%.3f, pop=%.2f, demand=%.2f",
        card["title"],
        tags,
        importance,
        popularity_score,
        topic_demand_score,
    )
    return card


def _insert_cards(cards: List[Dict[str, Any]]) -> None:
    """
    –í—Å—Ç–∞–≤–ª—è–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –≤ Supabase –ø–∞—á–∫–∞–º–∏.
    """
    if not cards:
        return

    total = len(cards)
    idx = 0
    while idx < total:
        batch = cards[idx : idx + SUPABASE_INSERT_BATCH_SIZE]
        resp = supabase.table("cards").insert(batch).execute()
        inserted = len(getattr(resp, "data", None) or getattr(resp, "model", None) or [])
        log.info("Inserted %d Wikipedia cards (batch size=%d)", inserted, len(batch))
        idx += SUPABASE_INSERT_BATCH_SIZE


def _select_cards_with_topic_limits(
    cards: List[Dict[str, Any]],
    max_total: int,
    per_topic_limit: int,
) -> List[Dict[str, Any]]:
    """
    –í—ã–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —É—á—ë—Ç–æ–º:
    - –æ–±—â–µ–≥–æ –ª–∏–º–∏—Ç–∞ max_total;
    - –ª–∏–º–∏—Ç–∞ per_topic_limit –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–µ–≥ –∏–∑ ALLOWED_TOPIC_TAGS.
    """
    topic_counts: Dict[str, int] = {t: 0 for t in ALLOWED_TOPIC_TAGS}
    selected: List[Dict[str, Any]] = []

    for card in cards:
        tags = card.get("tags") or []
        if not isinstance(tags, list):
            tags = [tags]

        canonical_tags = [t for t in tags if t in ALLOWED_TOPIC_TAGS_SET]
        if not canonical_tags:
            canonical_tags = ["world_news"]

        # –µ—Å–ª–∏ –ø–æ –≤—Å–µ–º —Ç–µ–≥–∞–º –∫–∞—Ä—Ç–æ—á–∫–∏ –ª–∏–º–∏—Ç —É–∂–µ –≤—ã–±—Ä–∞–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if all(topic_counts.get(t, 0) >= per_topic_limit for t in canonical_tags):
            continue

        selected.append(card)

        for t in canonical_tags:
            if topic_counts.get(t, 0) < per_topic_limit:
                topic_counts[t] = topic_counts.get(t, 0) + 1

        if len(selected) >= max_total:
            break

    log.info("Topic quotas after selection: %s", topic_counts)
    return selected


# ==========
# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
# ==========


def fetch_wikipedia_articles() -> None:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä:
    - –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (user_topic_weights),
    - –ø–æ –∫–∞–∂–¥–æ–º—É —è–∑—ã–∫—É (en/ru) –±–µ—Ä—ë—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π (seed + trending –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π),
    - –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏:
        - —Å—Ç—Ä–æ–∏—Ç URL,
        - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ—Ç –ª–∏ —É–∂–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —Ç–∞–∫–∏–º source_ref,
        - —Ç—è–Ω–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Wikipedia,
        - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –Ω–∞—à–µ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —É—á—ë—Ç–æ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–ø—Ä–æ—Å–∞,
    - —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ –ø–æ importance_score,
    - –≤ —Ä–µ–∂–∏–º–µ "bulk" –¥–æ–±–∏—Ä–∞–µ—Ç –æ–±—â–µ–µ —á–∏—Å–ª–æ wiki-–∫–∞—Ä—Ç –¥–æ ~WIKIPEDIA_BULK_TARGET_TOTAL,
    - –≤ —Ä–µ–∂–∏–º–µ "daily" –≤—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ ~50 –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–æ–ø–∏–∫ (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ —Ç–µ–≥–∞–º).
    """
    global_topic_demand = _load_global_topic_demand()
    if not global_topic_demand:
        log.info("Global topic demand is empty, wiki ingest will use content-based scoring only")

    prepared_cards: List[Dict[str, Any]] = []

    for lang in WIKIPEDIA_LANGS:
        log.info("Processing Wikipedia articles for lang=%s", lang)

        articles = _build_articles_for_lang(lang)
        if not articles:
            log.warning("No candidate articles for lang=%s, skipping", lang)
            continue

        max_views = max((a["views"] for a in articles), default=0)
        if max_views <= 0:
            max_views = 1

        for art in articles:
            title = art["title"]
            url = f"https://{lang}.wikipedia.org/wiki/{title}"

            if _card_exists(url):
                # –ù–µ –¥—É–±–ª–∏—Ä—É–µ–º —Ç–æ, —á—Ç–æ —É–∂–µ –µ—Å—Ç—å
                continue

            extract = _fetch_article_extract(lang, title)
            if not extract:
                continue

            # –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å: –¥–ª—è seed —Å views=0 –¥–∞—ë–º —É–º–µ—Ä–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ,
            # –¥–ª—è trending ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –º–∞–∫—Å–∏–º—É–º—É –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤.
            if art["views"] > 0:
                popularity_score = min(1.0, art["views"] / float(max_views))
            elif art.get("is_seed"):
                popularity_score = 0.5
            else:
                popularity_score = 0.2

            try:
                card = _normalize_to_card(
                    lang=lang,
                    title=title,
                    url=url,
                    extract=extract,
                    popularity_score=popularity_score,
                    global_topic_demand=global_topic_demand,
                )
            except Exception:
                log.exception(
                    "Failed to normalize Wikipedia article %s (%s)",
                    title,
                    lang,
                )
                continue

            if card:
                prepared_cards.append(card)

    if not prepared_cards:
        log.info("No Wikipedia cards prepared on this run")
        return

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ importance_score (—É—á–∏—Ç—ã–≤–∞–µ—Ç –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å, –∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã)
    prepared_cards.sort(
        key=lambda c: float(c.get("importance_score") or 0.0),
        reverse=True,
    )

    existing_total = _count_existing_wikipedia_cards()
    log.info(
        "Existing wikipedia cards in DB: %d (mode=%s)",
        existing_total,
        WIKIPEDIA_INGEST_MODE,
    )

    # --- –†–µ–∂–∏–º bulk: –¥–æ–±–∏—Ä–∞–µ–º –¥–æ WIKIPEDIA_BULK_TARGET_TOTAL --- #
    if WIKIPEDIA_INGEST_MODE == "bulk":
        bulk_target = WIKIPEDIA_BULK_TARGET_TOTAL if WIKIPEDIA_BULK_TARGET_TOTAL > 0 else 1000
        remaining = bulk_target - existing_total
        if remaining <= 0:
            log.info(
                "Bulk target already reached (target=%d, existing=%d). Nothing to do.",
                bulk_target,
                existing_total,
            )
            return

        max_total = min(remaining, len(prepared_cards))
        selected_cards = prepared_cards[:max_total]

        log.info(
            "Bulk mode: target=%d, existing=%d, this_run=%d",
            bulk_target,
            existing_total,
            len(selected_cards),
        )
        _insert_cards(selected_cards)
        log.info("Wikipedia bulk ingest finished, total cards inserted=%d", len(selected_cards))
        return

    # --- –û–±—ã—á–Ω—ã–π daily-—Ä–µ–∂–∏–º --- #
    # –¢—É—Ç –º—ã —Ö–æ—Ç–∏–º –ø—Ä–∏–º–µ—Ä–Ω–æ "–¥–æ 50 –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–æ–ø–∏–∫" –∑–∞ –∑–∞–ø—É—Å–∫.
    max_total_daily = min(WIKIPEDIA_MAX_CARDS_PER_RUN, len(prepared_cards))
    selected_cards = _select_cards_with_topic_limits(
        prepared_cards,
        max_total=max_total_daily,
        per_topic_limit=WIKIPEDIA_PER_TOPIC_DAILY_LIMIT,
    )

    if not selected_cards:
        log.info("No wikipedia cards selected after topic limits")
        return

    _insert_cards(selected_cards)
    log.info(
        "Wikipedia daily ingest finished, total cards inserted=%d (max_per_topic=%d, max_total=%d)",
        len(selected_cards),
        WIKIPEDIA_PER_TOPIC_DAILY_LIMIT,
        max_total_daily,
    )


def main() -> None:
    fetch_wikipedia_articles()


if __name__ == "__main__":
    main()
