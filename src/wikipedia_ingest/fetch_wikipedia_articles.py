# file: src/wikipedia_ingest/fetch_wikipedia_articles.py
import os
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

import requests
from supabase import Client, create_client
from dotenv import load_dotenv  # üîπ –¥–æ–±–∞–≤–∏–ª–∏

from webapp_backend.openai_client import normalize_telegram_post

log = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ==========
# Supabase
# ==========

# üîπ –≥—Ä—É–∑–∏–º .env (–∫–∞–∫ –≤ telegram_ingest)
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("SUPABASE_URL or SUPABASE_KEY is not set. Check your .env")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ==========
# –ö–æ–Ω—Ñ–∏–≥ Wikipedia / Wikimedia
# ==========

WIKIPEDIA_LANGS: List[str] = ["en", "ru"]

# –ë–∞–∑–æ–≤—ã–µ (seed) —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ trending –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
WIKIPEDIA_SEED_ARTICLES: Dict[str, List[str]] = {
    "en": [
        "Artificial_intelligence",
        "Startup_company",
        "Universities_in_the_United_Kingdom",
        "Streaming_media",
        "Climate_change",
    ],
    "ru": [
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π_–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
        "–°—Ç–∞—Ä—Ç–∞–ø",
        "–°–∏—Å—Ç–µ–º–∞_–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è_–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏–∏",
        "–ü–æ—Ç–æ–∫–æ–≤–æ–µ_–º—É–ª—å—Ç–∏–º–µ–¥–∏–∞",
        "–ò–∑–º–µ–Ω–µ–Ω–∏–µ_–∫–ª–∏–º–∞—Ç–∞",
    ],
}

# –ü—Ä–æ–µ–∫—Ç—ã –¥–ª—è Wikimedia API
WIKIMEDIA_PROJECTS: Dict[str, str] = {
    "en": "en.wikipedia.org",
    "ru": "ru.wikipedia.org",
}

# –ï—â—ë –Ω–µ–º–Ω–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–æ–≤
WIKIPEDIA_TRENDING_TITLES_PER_LANG = int(
    os.getenv("WIKIPEDIA_TRENDING_TITLES_PER_LANG", "20")
)

WIKIMEDIA_USER_AGENT = os.getenv(
    "WIKIMEDIA_USER_AGENT",
    "EYYE-MVP/0.1 (https://github.com/artemarnautov/eyye-tg-bot; contact: dev@eyye.local)",
)

WIKIMEDIA_TOP_URL_TEMPLATE = (
    "https://wikimedia.org/api/rest_v1/metrics/pageviews/top/"
    "{project}/all-access/{year}/{month}/all-days"
)

WIKIPEDIA_API_URL_TEMPLATE = "https://{lang}.wikipedia.org/w/api.php"

# –°–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤—Å—Ç–∞–≤–ª—è–µ–º –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
SUPABASE_INSERT_BATCH_SIZE = int(os.getenv("WIKIPEDIA_INSERT_BATCH_SIZE", "50"))

# ==========
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==========


def _card_exists(source_ref: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∫–∞—Ä—Ç–æ—á–∫–∞ —Å —Ç–∞–∫–∏–º source_type/source_ref.
    """
    resp = (
        supabase.table("cards")
        .select("id")
        .eq("source_type", "wikipedia")
        .eq("source_ref", source_ref)
        .limit(1)
        .execute()
    )
    data = resp.data or []
    return len(data) > 0


def _fetch_trending_titles_for_lang(lang: str) -> List[str]:
    """
    –ë–µ—Ä—ë–º —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ —Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü –∏–∑ Wikimedia Pageviews API.
    –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ª–æ–º–∞–µ—Ç—Å—è (403 –∏ —Ç.–ø.) ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –∞ –¥–∞–ª—å—à–µ
    –≤—ã—à–µ –ø–æ —Å—Ç–µ–∫—É —É–ø–∞–¥—ë–º –Ω–∞ seed-—Å—Ç–∞—Ç—å–∏.
    """
    project = WIKIMEDIA_PROJECTS.get(lang)
    if not project:
        log.warning("No Wikimedia project configured for lang=%s", lang)
        return []

    today = datetime.utcnow()
    url = WIKIMEDIA_TOP_URL_TEMPLATE.format(
        project=project,
        year=today.year,
        month=f"{today.month:02d}",
    )

    headers = {
        "User-Agent": WIKIMEDIA_USER_AGENT,
        "accept": "application/json",
    }

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        log.warning("Failed to fetch trending articles for %s: %s", lang, e)
        return []

    data = resp.json() or {}
    items = data.get("items") or []
    if not items:
        log.warning("No items in Wikimedia top response for lang=%s", lang)
        return []

    # –í –æ—Ç–≤–µ—Ç–µ items[0].articles ‚Äî —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
    first_item = items[0] or {}
    articles = first_item.get("articles") or []

    titles: List[str] = []
    for art in articles:
        title = art.get("article")
        if not isinstance(title, str):
            continue

        # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if title.startswith("Special:") or title.startswith("Main_Page"):
            continue

        titles.append(title)
        if len(titles) >= WIKIPEDIA_TRENDING_TITLES_PER_LANG:
            break

    log.info(
        "Fetched %d trending titles for lang=%s",
        len(titles),
        lang,
    )
    return titles


def _build_titles_for_lang(lang: str) -> List[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞:
    - seed-—Å—Ç–∞—Ç—å–∏ (AI, —Å—Ç–∞—Ä—Ç–∞–ø—ã, UK unis, —Å—Ç—Ä–∏–º–∏–Ω–≥, –∫–ª–∏–º–∞—Ç)
    - –ø–ª—é—Å trending –∏–∑ Wikimedia, –µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å –∏—Ö –ø–æ–ª—É—á–∏—Ç—å
    """
    titles: List[str] = []

    seed = WIKIPEDIA_SEED_ARTICLES.get(lang, [])
    titles.extend(seed)

    trending = _fetch_trending_titles_for_lang(lang)
    for t in trending:
        if t not in titles:
            titles.append(t)

    return titles


def _fetch_article_extract(lang: str, title: str) -> Optional[str]:
    """
    –¢—è–Ω–µ–º –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ Wikipedia API.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º prop=extracts, plaintext, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ –¥–ª–∏–Ω–µ.
    """
    api_url = WIKIPEDIA_API_URL_TEMPLATE.format(lang=lang)

    headers = {
        "User-Agent": WIKIMEDIA_USER_AGENT,
    }

    params = {
        "action": "query",
        "format": "json",
        "prop": "extracts",
        "explaintext": True,
        "exchars": 2000,  # –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–µ—Ä–≤—ã–µ ~2k —Å–∏–º–≤–æ–ª–æ–≤
        "redirects": 1,
        "titles": title,
    }

    try:
        resp = requests.get(api_url, headers=headers, params=params, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        log.warning(
            "Failed to fetch Wikipedia article '%s' (lang=%s): %s",
            title,
            lang,
            e,
        )
        return None

    data = resp.json() or {}
    pages = data.get("query", {}).get("pages", {})
    if not pages:
        log.warning("No pages in Wikipedia response for title=%r, lang=%s", title, lang)
        return None

    page = next(iter(pages.values()))
    extract = page.get("extract")
    if not extract or not str(extract).strip():
        log.warning("Empty extract for title=%r, lang=%s", title, lang)
        return None

    return str(extract)


def _normalize_to_card(
    lang: str,
    title: str,
    url: str,
    extract: str,
) -> Optional[Dict[str, Any]]:
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ–º —Ç–µ–∫—Å—Ç –í–∏–∫–∏ —á–µ—Ä–µ–∑ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π normalize_telegram_post,
    —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å title/body/tags/importance_score.

    –î–æ–ø. –ª–æ–≥–∏–∫–∞:
    - –µ—Å–ª–∏ source_name –æ—Ç –º–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 'wikipedia', –º—ã –µ–≥–æ –ø–µ—Ä–µ—Ç–∏—Ä–∞–µ–º –Ω–∞
      —á—Ç–æ-—Ç–æ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ, —á—Ç–æ–±—ã –Ω–µ –ø–∏—Å–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á—Ç–æ –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –í–∏–∫–∏–ø–µ–¥–∏—è.
    """
    # Lang –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏: –æ—Å—Ç–∞–≤–∏–º "en"/"ru", –∫–∞–∫ –∏ –µ—Å—Ç—å
    normalized = normalize_telegram_post(
        raw_text=extract,
        channel_title=f"Wikipedia ({lang})",
        language=lang,
    )

    tags = normalized.get("tags") or []
    if not isinstance(tags, list):
        tags = []

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ / —Ç–µ–ª–æ
    norm_title = (normalized.get("title") or "").strip()
    if not norm_title:
        norm_title = title.replace("_", " ")

    norm_body = (normalized.get("body") or "").strip()
    if not norm_body:
        # fallback: –∫—É—Å–æ–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        norm_body = extract[:800]

    # –í–∞–∂–Ω–æ—Å—Ç—å
    try:
        importance = float(normalized.get("importance_score", 0.5))
    except Exception:
        importance = 0.5

    # –ò—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ —Ö–æ—Ç–∏–º –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å–ª–æ–≤–æ "Wikipedia"
    source_name = (normalized.get("source_name") or "").strip()
    if not source_name or "wikipedia" in source_name.lower():
        source_name = "EYYE ‚Ä¢ AI-–ø–æ–¥–±–æ—Ä–∫–∞"

    card: Dict[str, Any] = {
        "title": norm_title,
        "body": norm_body,
        "tags": tags,
        "importance_score": importance,
        "language": "en" if lang == "en" else "ru",
        "is_active": True,
        "source_type": "wikipedia",
        "source_ref": url,
        "meta": {"source_name": source_name},
    }

    log.info(
        "Prepared Wikipedia card: title=%r, source_name=%r, tags=%r",
        card["title"],
        source_name,
        tags,
    )
    return card


def _insert_cards(cards: List[Dict[str, Any]]) -> None:
    """
    –í—Å—Ç–∞–≤–ª—è–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –≤ Supabase –ø–∞—á–∫–∞–º–∏.
    """
    if not cards:
        return

    total = len(cards)
    idx = 0
    while idx < total:
        batch = cards[idx : idx + SUPABASE_INSERT_BATCH_SIZE]
        resp = supabase.table("cards").insert(batch).execute()
        inserted = len(resp.data or [])
        log.info("Inserted %d Wikipedia cards (batch size=%d)", inserted, len(batch))
        idx += SUPABASE_INSERT_BATCH_SIZE


# ==========
# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
# ==========


def fetch_wikipedia_articles() -> None:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ—Ä–∫–µ—Ä:
    - –ø–æ –∫–∞–∂–¥–æ–º—É —è–∑—ã–∫—É (en/ru) –±–µ—Ä—ë—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π (seed + trending),
    - –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏:
        - —Å—Ç—Ä–æ–∏—Ç URL,
        - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ—Ç –ª–∏ —É–∂–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —Ç–∞–∫–∏–º source_ref,
        - —Ç—è–Ω–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Wikipedia,
        - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –Ω–∞—à–µ–π –∫–∞—Ä—Ç–æ—á–∫–∏,
        - –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏.
    """
    prepared_cards: List[Dict[str, Any]] = []

    for lang in WIKIPEDIA_LANGS:
        log.info("Processing Wikipedia articles for lang=%s", lang)

        titles = _build_titles_for_lang(lang)
        if not titles:
            log.warning("No titles for lang=%s, skipping", lang)
            continue

        for title in titles:
            url = f"https://{lang}.wikipedia.org/wiki/{title}"

            if _card_exists(url):
                # –ù–µ –¥—É–±–ª–∏—Ä—É–µ–º —Ç–æ, —á—Ç–æ —É–∂–µ –µ—Å—Ç—å
                continue

            extract = _fetch_article_extract(lang, title)
            if not extract:
                continue

            try:
                card = _normalize_to_card(lang, title, url, extract)
            except Exception:
                log.exception(
                    "Failed to normalize Wikipedia article %s (%s)",
                    title,
                    lang,
                )
                continue

            if card:
                prepared_cards.append(card)

    if not prepared_cards:
        log.info("No Wikipedia cards prepared on this run")
        return

    _insert_cards(prepared_cards)
    log.info("Wikipedia ingest finished, total cards prepared=%d", len(prepared_cards))


def main() -> None:
    fetch_wikipedia_articles()


if __name__ == "__main__":
    main()
