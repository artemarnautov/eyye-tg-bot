# file: infra/tg_channel_discovery/tgstat_scrape_top_channels.py

import json
import os
import re
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Set

import requests
from bs4 import BeautifulSoup, Tag

# ==============================
# –ö–æ–Ω—Ñ–∏–≥
# ==============================

# –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
MIN_SUBSCRIBERS = 40_000

# –º–∞–∫—Å–∏–º—É–º –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ —Ç–æ–ø–∏–∫
MAX_CHANNELS_PER_TOPIC = 5

# –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞
GLOBAL_BANNED_KEYWORDS = [
    # –∞–∑–∞—Ä—Ç–∫–∞ / —Å—Ç–∞–≤–∫–∏ / –∫–∞–∑–∏–Ω–æ
    "—Å—Ç–∞–≤–∫–∏", "–±–µ—Ç ", "bet ", "–±–∞–Ω–∫—Ä–æ–ª–ª", "–±—É–∫–º–µ–∫–µ—Ä", "–±—É–∫–º–µ–∫–µ—Ä—ã",
    "1xbet", "–≤–∏–Ω–ª–∞–π–Ω", "–ª–µ–æ–Ω–±–µ—Ç", "casino", "–∫–∞–∑–∏–Ω–æ",
    # –∫—Ä–∏–ø—Ç–∞-—à–ª–∞–∫ / —Ä–∞–∑–≤–æ–¥—ã
    "—Å–∏–≥–Ω–∞–ª—ã", "signal", "–ø–∞–º–ø", "–¥–∞–º–ø", "–∏–Ω—Å–∞–π–¥", "—Å–ª–∏–≤", "—Å–ª–∏–≤—ã",
    # –≤–∑—Ä–æ—Å–ª—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    "—ç—Ä–æ—Ç", "–ø–æ—Ä–Ω–æ", "porno", "xxx", "18+", "onlyfans", "nsfw",
    # –∏–Ω—Ñ–æ—Ü—ã–≥–∞–Ω—â–∏–Ω–∞
    "–∑–∞—Ä–∞–±–æ—Ç–∞–π", "–∑–∞—Ä–∞–±–æ—Ç–æ–∫ –±–µ–∑ –≤–ª–æ–∂–µ–Ω–∏–π", "–±—ã—Å—Ç—Ä—ã–π –∑–∞—Ä–∞–±–æ—Ç–æ–∫",
]

# –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å include/exclude –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–ø–∏–∫–∏
TOPIC_KEYWORDS_INCLUDE: Dict[str, List[str]] = {
    # –ø—Ä–∏–º–µ—Ä:
    # "business": ["–±–∏–∑–Ω–µ—Å", "—Å—Ç–∞—Ä—Ç–∞–ø", "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏"],
}

TOPIC_KEYWORDS_EXCLUDE: Dict[str, List[str]] = {}

# üî• –ö–õ–Æ–ß–ò –ó–î–ï–°–¨ = –¢–û–ß–ù–û –¢–í–û–ò id –ò–ó app.js
# URL'—ã ‚Äî —ç—Ç–æ –º–∞–ø–ø–∏–Ω–≥ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ TGStat (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
# –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–µ–π—Ç–∏–Ω–≥–æ–≤).
TOPIC_CONFIG: Dict[str, str] = {
    # –ú–∏—Ä
    "world_news":  "https://tgstat.ru/ratings/channels/news?sort=members",

    # –ë–∏–∑–Ω–µ—Å / –¥–µ–Ω—å–≥–∏
    "business":    "https://tgstat.ru/ratings/channels/business?sort=members",
    "finance":     "https://tgstat.ru/ratings/channels/economics?sort=members",  # —Ñ–∏–Ω–∞–Ω—Å—ã/—ç–∫–æ–Ω–æ–º–∏–∫–∞

    # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ / –Ω–∞—É–∫–∞ / –∏—Å—Ç–æ—Ä–∏—è
    "tech":        "https://tgstat.ru/ratings/channels/tech?sort=members",
    "science":     "https://tgstat.ru/ratings/channels/science?sort=members",
    "history":     "https://tgstat.ru/ratings/channels/history?sort=members",

    # –ü–æ–ª–∏—Ç–∏–∫–∞ / –æ–±—â–µ—Å—Ç–≤–æ
    "politics":    "https://tgstat.ru/ratings/channels/politics?sort=members",
    "society":     "https://tgstat.ru/ratings/channels/society?sort=members",

    # –ö–∏–Ω–æ / —Å–µ—Ä–∏–∞–ª—ã / —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
    "entertainment": "https://tgstat.ru/ratings/channels/cinema?sort=members",

    # –ò–≥—Ä—ã / —Å–ø–æ—Ä—Ç
    "gaming":      "https://tgstat.ru/ratings/channels/games?sort=members",
    "sports":      "https://tgstat.ru/ratings/channels/sport?sort=members",

    # –õ–∞–π—Ñ—Å—Ç–∞–π–ª
    "lifestyle":   "https://tgstat.ru/ratings/channels/lifestyle?sort=members",

    # –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ / –∫–∞—Ä—å–µ—Ä–∞
    "education":   "https://tgstat.ru/ratings/channels/education?sort=members",

    # –ì–æ—Ä–æ–¥ / –ª–æ–∫–∞–ª—å–Ω—ã–µ
    "city":        "https://tgstat.ru/ratings/channels/city?sort=members",  # –µ—Å–ª–∏ –Ω–µ—Ç, –∑–∞–º–µ–Ω–∏—à—å –Ω–∞ —Ä–∞–∑–¥–µ–ª –ª–æ–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π

    # –°—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è –∂–∏–∑–Ω—å –≤ UK ‚Äî –±–µ—Ä—ë–º –æ–±—â—É—é education,
    # –ø–æ—Ç–æ–º –≤—Ä—É—á–Ω—É—é/—á–µ—Ä–µ–∑ —Ñ–∏–ª—å—Ç—Ä—ã –æ—Å—Ç–∞–≤–∏–º UK/—É–Ω–∏–≤–µ—Ä—ã
    "uk_students": "https://tgstat.ru/ratings/channels/education?sort=members",
}

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/121.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru,en;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Connection": "keep-alive",
}

OUTPUT_PATH = os.path.join("data", "tg_channels_seed.json")

# ==============================
# –ú–æ–¥–µ–ª—å
# ==============================

@dataclass
class Channel:
    title: str
    username: str
    url: str
    subscribers: int
    topic: str

# ==============================
# –ü–∞—Ä—Å–∏–Ω–≥
# ==============================

SUBS_PATTERNS = [
    re.compile(r"([\d\s\u00A0]+)\s+–ø–æ–¥–ø–∏—Å—á–∏–∫", re.IGNORECASE),
    re.compile(r"([\d\s\u00A0]+)\s+subscribers?", re.IGNORECASE),
]


def parse_subscribers(text: str) -> Optional[int]:
    for pattern in SUBS_PATTERNS:
        m = pattern.search(text)
        if m:
            raw = m.group(1)
            digits = raw.replace(" ", "").replace("\u00A0", "")
            if digits.isdigit():
                return int(digits)
    return None


def text_contains_any(text: str, words: List[str]) -> bool:
    t = text.lower()
    return any(w.lower() in t for w in words)


def is_channel_allowed(channel: Channel, block_text: str) -> bool:
    text = f"{channel.title} {block_text}".lower()

    # –≥–ª–æ–±–∞–ª—å–Ω—ã–π –º—É—Å–æ—Ä
    if text_contains_any(text, GLOBAL_BANNED_KEYWORDS):
        return False

    # –º–∏–Ω–∏–º—É–º –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
    if channel.subscribers < MIN_SUBSCRIBERS:
        return False

    # —Ç–æ–ø–∏–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ include/exclude
    topic_includes = TOPIC_KEYWORDS_INCLUDE.get(channel.topic)
    topic_excludes = TOPIC_KEYWORDS_EXCLUDE.get(channel.topic)

    if topic_excludes and text_contains_any(text, topic_excludes):
        return False

    if topic_includes and not text_contains_any(text, topic_includes):
        return False

    return True


def extract_channel_blocks(soup: BeautifulSoup) -> List[Tag]:
    """
    –ù–∞—Ö–æ–¥–∏–º HTML-–±–ª–æ–∫–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ t.me.
    –ù–µ –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ–º—Å—è –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∫–ª–∞—Å—Å–∞–º ‚Äî –±–µ—Ä—ë–º —Ä–æ–¥–∏—Ç–µ–ª—è <tr> –∏–ª–∏ <div>.
    """
    blocks: List[Tag] = []
    seen: Set[int] = set()

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "t.me/" not in href:
            continue

        block = a.find_parent("tr") or a.find_parent("div") or a.parent
        if not isinstance(block, Tag):
            continue

        bid = id(block)
        if bid in seen:
            continue
        seen.add(bid)
        blocks.append(block)

    return blocks


def parse_block_to_channel(block: Tag, topic: str) -> Optional[Channel]:
    link = None
    for a in block.find_all("a", href=True):
        if "t.me/" in a["href"]:
            link = a
            break

    if link is None:
        return None

    url = link["href"]
    title = link.get_text(strip=True) or url

    m = re.search(r"t\.me/([\w\d_]+)", url)
    if not m:
        return None
    username = m.group(1)

    block_text = " ".join(block.stripped_strings)
    subs = parse_subscribers(block_text)
    if subs is None:
        return None

    return Channel(
        title=title,
        username=username,
        url=url,
        subscribers=subs,
        topic=topic,
    )

# ==============================
# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ==============================

def scrape_topic(topic: str, url: str) -> List[Channel]:
    print(f"\n=== –¢–æ–ø–∏–∫: {topic} ===")
    print(f"URL: {url}")

    resp = requests.get(url, headers=DEFAULT_HEADERS, timeout=30)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    blocks = extract_channel_blocks(soup)
    print(f"–ù–∞–π–¥–µ–Ω–æ HTML-–±–ª–æ–∫–æ–≤ —Å t.me: {len(blocks)}")

    channels: List[Channel] = []
    seen_usernames: Set[str] = set()

    for block in blocks:
        ch = parse_block_to_channel(block, topic)
        if ch is None:
            continue
        if ch.username in seen_usernames:
            continue

        block_text = " ".join(block.stripped_strings)
        if not is_channel_allowed(ch, block_text):
            continue

        seen_usernames.add(ch.username)
        channels.append(ch)

    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º –∏ –±–µ—Ä—ë–º —Ç–æ–ø N
    channels.sort(key=lambda c: c.subscribers, reverse=True)
    selected = channels[:MAX_CHANNELS_PER_TOPIC]

    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {len(selected)} (>= {MIN_SUBSCRIBERS} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤)")
    for c in selected:
        print(f"- {c.title} (@{c.username}) ‚Äî {c.subscribers}")

    return selected


def main() -> None:
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)

    result: Dict[str, List[Dict]] = {}

    for topic, url in TOPIC_CONFIG.items():
        try:
            channels = scrape_topic(topic, url)
            result[topic] = [asdict(ch) for ch in channels]
        except Exception as e:
            print(f"[ERROR] –¢–æ–ø–∏–∫ {topic}: {e}")
            result[topic] = []

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\n–ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
